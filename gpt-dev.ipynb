{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e081fadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e27e9cc6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640ea277",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277eb8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters is: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of characters is: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf9b592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6efc979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 e\n",
      "31 d\n",
      "32  \n",
      "33 a\n",
      "34 n\n",
      "35 y\n",
      "36  \n",
      "37 f\n",
      "38 u\n",
      "39 r\n",
      "40 t\n",
      "41 h\n",
      "42 e\n",
      "43 r\n",
      "44 ,\n",
      "45  \n",
      "46 h\n",
      "47 e\n",
      "48 a\n",
      "49 r\n",
      "50  \n",
      "51 m\n",
      "52 e\n",
      "53  \n",
      "54 s\n",
      "55 p\n",
      "56 e\n",
      "57 a\n",
      "58 k\n",
      "59 .\n",
      "60 \n",
      "\n",
      "61 \n",
      "\n",
      "62 A\n",
      "63 l\n",
      "64 l\n",
      "65 :\n",
      "66 \n",
      "\n",
      "67 S\n",
      "68 p\n",
      "69 e\n"
     ]
    }
   ],
   "source": [
    "for i in range(30, 70):\n",
    "    print(i,text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1212ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5ba650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f48d412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "s_to_i = {s:i for i,s in enumerate(chars)}\n",
    "i_to_s = {i:s for i,s in enumerate(chars)}\n",
    "encode = lambda s: [s_to_i[c] for c in s]\n",
    "decode = lambda l: ''.join([i_to_s[i] for i in l])\n",
    "\n",
    "print(encode(\"hello there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba7d550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840d36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26da33f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is 47.\n",
      "When input is tensor([18, 47]) the target is 56.\n",
      "When input is tensor([18, 47, 56]) the target is 57.\n",
      "When input is tensor([18, 47, 56, 57]) the target is 58.\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is 1.\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is 15.\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47.\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58.\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target is {target}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d2f01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1337)\n",
    "# batch_size = 4\n",
    "# block_size = 8\n",
    "\n",
    "# def get_batch(split):\n",
    "#     data = train_data if split == 'train' else val_data\n",
    "#     ix = torch.randint(len(data) - block_size, (batch_size,)) #[num, num, ... 8 times]\n",
    "#     x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "#     y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "#     return x,y\n",
    "\n",
    "# xb, yb = get_batch('train')\n",
    "# print(f'Inputs: {xb} \\nInput Shape: {xb.shape}')\n",
    "# print(f\"Targets: {yb} \\nTarget Shape: {yb.shape}\")\n",
    "# print('-------')\n",
    "\n",
    "# for b in range(batch_size): #batch dimension\n",
    "#     for t in range(block_size): #time dimension\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b,t]\n",
    "#         print(f\"When input is {context.tolist()} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85528bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "batch_size = 64       #no. of independent seq we process in parallel\n",
    "block_size = 256      #max context length for pred\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca8fde2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #[num, num, ... 8 times]\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y =  x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "@torch.no_grad() # don't call .backward when doing estimate_loss()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37bf2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    #one head of self-attention\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__() #nn.Module's __init__\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #lower triangular matrix of ones\n",
    "        self.dropout = nn.Dropout(dropout) #think of dropout as a regularization/ optimization technique (kind of like random forest)\n",
    "\n",
    "    def forward(self, x): #in karpathy's tutorials he assumes C and head_size are same but C is actually embedding dimension\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #(B,T,head_size)\n",
    "        q = self.query(x) #(B,T,head_size)\n",
    "        wei = q @ k.transpose(-2,-1) * (k.shape[-1] **-0.5) #(B,T,head_size) @ (B,head_size,T) --> (B,T,T) *C**-0.5 is done to ensure that before softmax the variance is 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B,T,T) -inf so that softmax outputs 0 for them\n",
    "        wei = F.softmax(wei, dim=-1) #(B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) #(B,T,head_size)\n",
    "        out = wei @ v #(B,T,head_size)\n",
    "        #so out[0,2,7] means for the first batch the 3rd letter 8th dimension is the weighted sum of 8th dimesion of the value vectors for letters 0,1,2 \n",
    "        #where the weights are computed using the query of token 2 and keys of tokens 0-2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a5875",
   "metadata": {},
   "source": [
    "Multi-head attention lets the model attend to different types of relationships simultaneously, which a single large attention head fundamentally cannot do (even though the number of dimensions match) because single head produces only one attention atrix and just one attention matrix cannout represent multiple independent attention patterns.\n",
    "\n",
    "All the dimensions (say 32) of each letter get the same attention (or weights) in self attention but by dividng it into multiple heads of 8 dims each we can give these 4 batches of 8 dims different sets of attentions (or weights).\n",
    "\n",
    "Assume that first head is for syntax and second head is for semantics. Say letter 3 must attend strongly (say 0.75) for syntax and weakly (say 0.03) for semantics to letter 1 and strongly (say 0.70) to letter 1 for semantics and weakly (say 0.05) for syntax to letter 2. This won't be possible in single head of 32 because the entire letter (or all 32 dimensions) will get attented to by the same weight and the sum of weights of syntax attention and semantics attention would be greater than 1 which is not possible because of softmaxing all 32 dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e971e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    #multiple heads of self-attention in parallel\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__() #nn.Module's __init__\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) #this is done so that the concatenated head outputs back into the model's working \n",
    "        #embedding space so that we can add each dimension to x's dimension while adding residual connections (to prevent adding RGB to HSV)\n",
    "        self.dropout = nn.Dropout(dropout) #think of dropout as a regularization/ optimization technique (kind of like random forest)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb81a5",
   "metadata": {},
   "source": [
    "Self-attention decides how tokens communicate, but the feed-forward network is what actually computes meaningful representations from that information.\n",
    "\n",
    "In other words: Self-attention gathers information from other tokens, while the feed-forward network turns that information into useful features (this mainly happens because of the presence of non linearity in ffn). After FFN the token would have interpreted the context it received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31d845e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    #a linear layer followed by a non-linearity\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__() #nn.Module's __init__\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd), nn.ReLU(), nn.Linear(4*n_embd, n_embd),nn.Dropout(dropout),) #think of dropout as a regularization/ optimization technique (kind of like random forest)\n",
    "        #the last linear layer is what allows the FFN to recombine the ReLU-activated features into a new learned representation that can be added back via the residual connection\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aac77c",
   "metadata": {},
   "source": [
    "We need to add residual connections because it ensures that gradients cannot vanish completely and information always flows backward.\n",
    "\n",
    "Early in training, attention weights are nar-random and wihtout residuals, representations can collapse. Rediduals ensure that each layer can only improve or slightly adjust the representation.\n",
    "\n",
    "Residuals therefore help with stable optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa30dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    #Transformer block: communication followed by computation\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__() #nn.Module's __init__\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd) #normalises overall the dimensions for 1 token\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) #the x + is to add the residual\n",
    "        x = x + self.ffwd(self.ln1(x)) #same as above\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72deae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384, 65])\n",
      "tensor(4.3375, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() #nn.Module's __init__\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # self.sa_heads = MultiHeadAttention(4,n_embd//4) #i.e., 4 heads of 8-dimensional self-attention\n",
    "        # self.ffwd = FeedForward(n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) #final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C) (number of batches or sequences in parallel, number of tokens being processed, embedding dimension)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #(T,C)\n",
    "        x = tok_emb + pos_emb #(B,T,C)\n",
    "        # x = self.sa_heads(x) #apply one head of self-attention (B,T,C)\n",
    "        # x = self.ffwd(x) #(B,T,C)\n",
    "        x = self.blocks(x) #(B,T,C)\n",
    "        x = self.ln_f(x) #(B,T,C)\n",
    "        logits = self.lm_head(x) #(B,T,C) here C is vocab size. C was 32 earlier now it is 65 as we want logits here\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond) #equivalent to self.forward(idx, targets=None) so (B,T,C)\n",
    "            logits = logits[:, -1, :] #select last timestamp so it is (B,C), Given the seq so far, how likely is each possible next token\n",
    "            probs = F.softmax(logits, dim=-1) #(B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "logits, loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ada496fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.3358, val loss 4.3323\n",
      "Step 500: train loss 1.8958, val loss 2.0136\n",
      "Step 1000: train loss 1.5430, val loss 1.7318\n",
      "Step 1500: train loss 1.3993, val loss 1.6093\n",
      "Step 2000: train loss 1.3112, val loss 1.5537\n",
      "Step 2500: train loss 1.2550, val loss 1.5269\n",
      "Step 3000: train loss 1.2050, val loss 1.5071\n",
      "Step 3500: train loss 1.1623, val loss 1.4891\n",
      "Step 4000: train loss 1.1263, val loss 1.4872\n",
      "Step 4500: train loss 1.0900, val loss 1.4830\n",
      "\n",
      "Brings in this pectition is he sab now\n",
      "The ground: thou, sit is thou comest any brade,\n",
      "Yet thou art Ned'st, rootmage.\n",
      "\n",
      "GREMIO:\n",
      "But when thou promisest what we dream Was.\n",
      "And is some fairest petition.\n",
      "And have thoughts, with the stewder'd wild speak:\n",
      "That 'twento may speak thee must weach to pale.\n",
      "But what they dead, that thou toak again\n",
      "In our corse will not will again; but 'twere\n",
      "This many consense on your ghost love,\n",
      "That to be his higherTy, tragger, and yonder Wootman,\n",
      "His tongue whose cousin\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "     #every now and then evaluate the loss on train and val sets\n",
    "     if iter % eval_interval == 0:\n",
    "          losses = estimate_loss()\n",
    "          print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "     xb, yb = get_batch('train')\n",
    "     logits, loss = model(xb, yb)\n",
    "     optimizer.zero_grad(set_to_none = True)\n",
    "     loss.backward()\n",
    "     optimizer.step()\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist())) #[0] becasue we are taking the first element from generate's batch dim (albeit the batch dimension is only 1 rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9b686e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With truth his trabunes; like a wight posservative\n",
      "For a madam in a bred, osy steed where my flies,\n",
      "The torch from my son, our sweetless hand!\n",
      "O beat o'er my life by thine, wife, Wail I kill himself,\n",
      "So strok thy lips and course; he, havin\n",
      "Of they'll present thee most pay on thee.\n",
      "\n",
      "ESCALUS:\n",
      "Here in thy nature: 'tis a former sword.\n",
      "\n",
      "KING RICHARD II:\n",
      "Warriors, what plainly I now, wife.\n",
      "Which is the soldier, to close the lire:\n",
      "He is the fear'st o'erlop'd it as boldly.\n",
      "\n",
      "KING RICHARD II:\n",
      "I, in what cousin's head!\n",
      "\n",
      "SICINGENNEN:\n",
      "Then give some franks, so, then quickly meet,\n",
      "For let them  be a most thing your wealt\n",
      "Infery the subjects of lalls. Perlaisned ANNE:\n",
      "Where and I reposed for ages, yea, who like himself\n",
      "From elder the mirthstering of my costage\n",
      "Than Purgs 'shall young black you find yourself\n",
      "Is pretty this flishmes no room this rest.\n",
      "For many business that wrath waste your foe,--\n",
      "For your arms, and know yours hands, not easy!\n",
      "Commend me thou I accept of my stars,\n",
      "Concerns I am alread.\n",
      "In pertressage lengthening people, hath tellesd'd one\n",
      "Before I should wish me to be people;\n",
      "For inteeming, and tern I can recover the prevaile;\n",
      "So centinues all the obstance of the sea?\n",
      "\n",
      "First Senator:\n",
      "Wicked your sovereign, and we thee, swear; you fetch no negler;\n",
      "For so, 'tis beck; which we do noted, tears being\n",
      "Expless it with what vow Capplexion's pomp'd\n",
      "Wheness their lordships agabe: those eyess flond,\n",
      "The people indeed most planters birds, altering ands;\n",
      "Though now plain, on their ladys.\n",
      "We will return all at onceit with the air\n",
      "That lieutent to straight for my graves breeding very,\n",
      "Had close unmore roasing 's grewithal pime hence,\n",
      "But nor years like a grave with his friend beggar:\n",
      "And to three jointies like a gass,\n",
      "To care fight thee, are Sirah, lesters; Edward with thee:\n",
      "Bish thee he that of blames meet more shame\n",
      "Than is Rutlard's paraced like and new,\n",
      "The child, look though thou rose so stoproud,\n",
      "Or is soldier's flag Barnet, madam, obbscur age;\n",
      "Be quick and Heart's at odd them Moltly of Boling.\n",
      "As sorrow-coward, this 'tis subject my soul's pale.\n",
      "\n",
      "QUEEN:\n",
      "Supper have, whom thou carest?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Stay, let's dear down.\n",
      "\n",
      "RIVERS:\n",
      "And the gods the baby that stocks being and cap,\n",
      "This too grave have and publl'd with schilf.\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "Good bad, gentle Paris: would thou, ha? why, thence were no build,\n",
      "There's those were some the deadly leised in our hand,\n",
      "But strew means might for my tongue: holds\n",
      "From my death by the corse, thou hast kill a more\n",
      "I born have the wounds of hove.\n",
      "\n",
      "KING HENRY VI:\n",
      "O, but well not seeinly speak again,\n",
      "For a good but words again: such a blowers whence\n",
      "I cannot be respect for a loss, young But reach,\n",
      "A-garl; but the sugal rennels sudden: still\n",
      "Buildren at the eye heard now;\n",
      "For meantain doing that widow, why lets with a verse\n",
      "And power her lasters had lady, nurse'd\n",
      "Read, it say, 'Fore four Christian is note to\n",
      "The cherks of the queen: let's made had was feast:\n",
      "Would you'll hath remembers us, who we and,\n",
      "Screature his conquerous'd.\n",
      "\n",
      "PRINCE EGEDWIS:\n",
      "York what hadst our way, now\n",
      "To bear the bootliss for the world--twent tears?\n",
      "And till the suit, it ward's alarge for our life,\n",
      "And fall banish drops of itself climation\n",
      "That we may e'e belike us: but no more drunk,\n",
      "Like at such to delightness of the spirits.\n",
      "\n",
      "LADY CAPULET:\n",
      "See, murder, drink, that say as havago.\n",
      "\n",
      "Nurse:\n",
      "I'll suck me noar for. Thou comest me, Marcius, nor:\n",
      "How came it your senges to thee, mercy will their hopes.\n",
      "\n",
      "YORK:\n",
      "Cet now, by why so, O Earl to Montague!\n",
      "\n",
      "CANUS:\n",
      "Marry, march, thou sighst me in so, fowl we shore!\n",
      "Welcome, we will recoted than drack to this biggot:\n",
      "Why, as the murden, as we see us now as Among,\n",
      "but ones: lets no mine of it,--\n",
      "\n",
      "ESCALUS:\n",
      "To give me both, let each or such a knave\n",
      "Or at fair fence with an unweister's blood\n",
      "And that I would Camillo be slaughteous;\n",
      "And who paid strain his face and did hollow ea\n",
      "Recemped at have, than when his stall be steal'd\n",
      "As he both'd lad true from great son; the easeness can\n",
      "Time talking drew of Royalty\n",
      "Will we weep our wealing of the acceptation,\n",
      "So bod their pass abhorr'd thine boists of posters\n",
      "With noble eagle'st cupbuising\n",
      "And the chief tembans thousakes; the rough and please to eter,\n",
      "Neglect the knot one coulds forawly live.\n",
      "Bull same hither stand no more be wept on:\n",
      "Drought to be soon but speak off;\n",
      "So silk the wounds as comfort, as that it foll\n",
      "The scapes with gainst the fall of snow;\n",
      "And leave thee mock'd with with walful treasons, hold\n",
      "Whose justice at the through reading of here\n",
      "That consuls be rough: and swise, good will discharition\n",
      "Be with a surfeit traitor than gods past kelep\n",
      "Like, and friends what repented to him.\n",
      "Would there I had been digned, drown'd even,\n",
      "Be firew the sun for death.\n",
      "Now might clean falcons, couble lord may go;\n",
      "Such prince temples hum with dismal tread you:\n",
      "Have wife, for leave me to you, but a knowerful\n",
      "Was near thou be constablew mildly wretch!\n",
      "\n",
      "ThERBEO:\n",
      "We want forgover whence and captions there, Tarley;\n",
      "I am not nighty vow, by the city,\n",
      "And follows weighing in ourself audiss.\n",
      "Never Harry:\n",
      "Had no lords the seeks, we say; Which fie,\n",
      "We see him, decrees freech, with more once,\n",
      "For grave's palace, fierce many repass:\n",
      "Pronounce thy care that vantage them; wrong on\n",
      "Prefer fame Richard are from the state;\n",
      "Which we are join wide thou slay to knows,\n",
      "Since hath fooler, take the fully eagle of thy pearts.\n",
      "\n",
      "PERDITA:\n",
      "Mother, dangrow's are thou there bastard\n",
      "As shall their breaths; let them common hate.\n",
      "That thou I have phrchion Marcius'd commandmen'd;\n",
      "'I dared now we in flamour told the recover\n",
      "Is forfeit a part of peace. Pray you, sir, know your sweet returns\n",
      "To ungree amament. Signified\n",
      "Where he were at needful lies for wontely.\n",
      "\n",
      "Gard:\n",
      "This is not haught I patience the othing.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "This is no knowledge morn.\n",
      "\n",
      "Provost:\n",
      "Where it bad cut stays much lives about\n",
      "Thither shall be hunder, black'd in sump.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "'Twas none toe such offences; for, when now let the\n",
      "hird or watery gentle, and of a blood, he is in less\n",
      "Of golded at his brain's pardon tears for,\n",
      "She is her be prophecing: and let come remembrace\n",
      "For rest no countermand cheeks my curse and no.\n",
      "But repost this lack of the woman: he no more worst\n",
      "I let fain'd a thing\n",
      "So have than beat sweened you.\n",
      "\n",
      "SICINIUS:\n",
      "What,\n",
      "No, we must was your people,\n",
      "Ours, we have loved along on't!\n",
      "Cannot this not to hope nobility.\n",
      "\n",
      "VOLUMNIA:\n",
      "This to great me! Take her be goved: whom, gentle friend!\n",
      "\n",
      "VOLUMNIA:\n",
      "O, prepare when the sacrave butt dead!\n",
      "\n",
      "CORIOLANUS:\n",
      "I am plantage.\n",
      "\n",
      "AUFIDIUS:\n",
      "\n",
      "CORIOLANUS:\n",
      "Hence him in\n",
      "Till I make him by the holmost by mesiders\n",
      "And pity himself.\n",
      "\n",
      "Constable:\n",
      "Claid, look, again!\n",
      "\n",
      "VOLUNIA:\n",
      "You did obstro, add the plaintain of this impossible:\n",
      "More bitiness so much ignodion of the trest,\n",
      "And full welcome newly pardon and beater my love;\n",
      "By heavy sighs that Were wet my slay may.\n",
      "\n",
      "VALRIA:\n",
      "This chiefes or what are of my means in French.\n",
      "The morning revenge years their you met desperate!\n",
      "My heart Angelo, Jusse 'twas nobly, stars,\n",
      "His froppiness this shadows serves and crowning fair\n",
      "Aboth for the willing draws'd and man's black'd breelials;\n",
      "Lest him judge in: upon Engly's growing day\n",
      "Million foolish well to endown the fault:\n",
      "'Signior Bona, glib out the sessorrow,\n",
      "And you should live the common strangely bring to-morrow.\n",
      "Thus eyes this king in the moral, deal grave:\n",
      "I would more vive to where and her I say.\n",
      "And, she, poor he is a fext:\n",
      "What is it that, or the wild of heaven,\n",
      "They will from straging their bustelts vow,\n",
      "Whose if to their streepses. Wilt we say green Easure\n",
      "To suspect this duty anoner.\n",
      "\n",
      "SICINIUS:\n",
      "Go, go; for the orackness are words.\n",
      "\n",
      "Remain:\n",
      "To consay the house well.\n",
      "The chape you arr'd straight together, that loves to me\n",
      "For Richard: our wrongs with the forcement, for your golden\n",
      "Be very in a full of John\n",
      "That of Rome thus: we how now, thence, we lie,\n",
      "Were Lurk'd a more of ona.\n",
      "\n",
      "BENVOLIO:\n",
      "Speak, we will we still.\n",
      "\n",
      "MOPSA:\n",
      "Hence, but sweet, as well you will answer.\n",
      "\n",
      "LUCIO:\n",
      "You remember.\n",
      "\n",
      "POLINES:\n",
      "O two doublot God, that's good growan it.\n",
      "\n",
      "Second Citizen:\n",
      "My gracious, because you say 'Mistress will be in crosses;\n",
      "We cry, come and made no feck for war:\n",
      "We bid this unfold your gense crild him part.\n",
      "\n",
      "LUCIO:\n",
      "How now! what a worrancable? a calmasure?\n",
      "\n",
      "AUTOLYCUS:\n",
      "'Tis a fouller over the toucher:\n",
      "Why says 'have't? 'tis met be mark'd to come\n",
      "The haughter of my colours.\n",
      "\n",
      "CLAUDIO:\n",
      "Our more devilents by our own or rob,\n",
      "Snowing the feasy outrage gates! We will be masters\n",
      "Take a teeth craft of scene.\n",
      "\n",
      "CAMILLO:\n",
      "She comes she. for a dream o' the war,\n",
      "Can you hope, our roarful bosom's spirit off.\n",
      "\n",
      "CAPULET:\n",
      "O, I come a suddenly stand; and like but a cright;\n",
      "Stands yea, draw you make me you! for you little kiss,\n",
      "And smile my auther blanks, and you be not\n",
      "Come to the stroke, home he may live-hearted.\n",
      "\n",
      "PAULINA:\n",
      "\n",
      "MARIANA:\n",
      "Thou do seek back appliant.\n",
      "\n",
      "POMPEY:\n",
      "O, sir, call home, you are do some goodly sir.\n",
      "\n",
      "GLOUCESTER:\n",
      "The what allight should no, may few.\n",
      "Second me, Paris, a rough toad and lap down.\n",
      "\n",
      "Province:\n",
      "Above'd lead.\n",
      "\n",
      "KING EDWARD IV:\n",
      "This territator, whop then, I are thee dead.\n",
      "\n",
      "GLOUCESTER:\n",
      "But thou saw her laid so shrewd thee.\n",
      "\n",
      "GLOUCESTER:\n",
      "Let me before thy heavil women will be srow.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Give me thy head thou began thee with my out.\n",
      "\n",
      "SAMPUGHBY:\n",
      "On faither all his party, I do speak, Lord Salisbury,\n",
      "High quarrel and kneel carse, his micklife up with news.\n",
      "The digs I wall not cramour'd the wiveous old,\n",
      "And the sin of ny the causer will I from\n",
      "The Tlover than males than the deeph gaze upon.\n",
      "\n",
      "First Servant:\n",
      "Fares your kingly speak for Romeo: more comes.\n",
      "\n",
      "Second Servingman:\n",
      "Why?\n",
      "\n",
      "Second Keeper:\n",
      "And he speak upon how pass it, madam me must desires\n",
      "Hath told you, takewill your from him, let him be\n",
      "Three committed tilts may I will maintain;'\n",
      "Take't thou and find my cousin Lady Proceed:\n",
      "Honour's lords, and said 'Do been bours,\n",
      "But thy oath, most sovereign deep'd, with all,\n",
      "He smoody with this desperate and coals\n",
      "Because thy measure singulate\n",
      "My soul, and draw not gaunt me with ange\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist())) #[0] becasue we are taking the first element from generate's batch dim (albeit the batch dimension is only 1 rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad38957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for steps in range(10000):\n",
    "#     xb, yb = get_batch('train')\n",
    "#     logits, loss = m(xb,yb)\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "#     loss.backward()\n",
    "#     optimizer.step() #upgrade gradient\n",
    "# print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open3d_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
